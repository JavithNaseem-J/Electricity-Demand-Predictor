{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d21fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a82b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b945b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4d2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from electron.utils.helpers import *\n",
    "from electron.utils.exception import *\n",
    "from electron.constants import *\n",
    "from electron import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_file: Path\n",
    "    status_file: str\n",
    "    label_encoder: Path\n",
    "    preprocessor: Path\n",
    "    x_transform: Path\n",
    "    y_transform: Path\n",
    "    train_features: Path\n",
    "    test_features: Path\n",
    "    train_target: Path\n",
    "    test_target: Path\n",
    "    input_seq_len: int\n",
    "    step_size: int\n",
    "    cutoff_date: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_PATH,\n",
    "                       params_filepath=PARAMS_PATH,\n",
    "                       schema_filepath=SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        schema = self.schema\n",
    "        params = self.params.transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_file=Path(config.data_file),\n",
    "            status_file=Path(config.status_file),\n",
    "            label_encoder=Path(config.label_encoder),\n",
    "            preprocessor=Path(config.preprocessor),\n",
    "            x_transform=Path(config.x_transform),\n",
    "            y_transform=Path(config.y_transform),\n",
    "            train_features=Path(config.train_features),\n",
    "            test_features=Path(config.test_features),\n",
    "            train_target=Path(config.train_target),\n",
    "            test_target=Path(config.test_target),\n",
    "            input_seq_len=params.input_seq_len,\n",
    "            step_size=params.step_size,\n",
    "            cutoff_date=params.cutoff_date\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bdf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def check_status(self):\n",
    "        try:\n",
    "            with open(self.config.status_file, 'r') as f:\n",
    "                status_data = json.load(f)\n",
    "            validation_status = status_data.get(\"Validation status\", False)\n",
    "            logger.info(f\"Data validation status: {validation_status}\")\n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading validation status: {e}\")\n",
    "            return False\n",
    "\n",
    "    def basic_preprocessing(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.data_file)\n",
    "            df = df[['period', 'subba', 'value', 'temperature_2m']]\n",
    "            le = LabelEncoder()\n",
    "            df['sub_region_code'] = le.fit_transform(df['subba'])\n",
    "\n",
    "            df.rename(columns={\n",
    "                'period': 'date',\n",
    "                'subba': 'sub_region',\n",
    "                'value': 'demand'\n",
    "            }, inplace=True)\n",
    "\n",
    "            df = df[['date', 'sub_region_code', 'demand', 'temperature_2m']]\n",
    "\n",
    "            create_directories([os.path.dirname(self.config.label_encoder)])\n",
    "            save_bin(le, self.config.label_encoder)\n",
    "\n",
    "            logger.info(\"Basic preprocessing completed.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True)\n",
    "\n",
    "            df['hour'] = df['date'].dt.hour\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "            holidays = calendar().holidays(start=df['date'].min(), end=df['date'].max())\n",
    "            df['is_holiday'] = df['date'].isin(holidays).astype(int)\n",
    "\n",
    "            logger.info(\"Feature engineering completed.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def train_test_splitting(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        try:\n",
    "            df = self.feature_engineering(self.basic_preprocessing())\n",
    "            df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "            cutoff = pd.to_datetime(self.config.cutoff_date, utc=True)\n",
    "\n",
    "            train_df = df[df['date'] < cutoff].reset_index(drop=True)\n",
    "            test_df = df[df['date'] >= cutoff].reset_index(drop=True)\n",
    "\n",
    "            logger.info(f\"Train size: {train_df.shape}, Test size: {test_df.shape}\")\n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def _get_cutoff_indices(self, df: pd.DataFrame, input_seq_len: int, step_size: int):\n",
    "        stop = len(df) - input_seq_len - 1\n",
    "        return [(i, i + input_seq_len, i + input_seq_len + 1) for i in range(0, stop, step_size)]\n",
    "        \n",
    "    def transform_ts_data_into_features_and_target(self, ts_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "            \n",
    "            assert set(['date', 'demand', 'sub_region_code', 'temperature_2m']).issubset(ts_data.columns)\n",
    "\n",
    "            region_codes = ts_data['sub_region_code'].unique()\n",
    "            features = pd.DataFrame()\n",
    "            targets = pd.DataFrame()\n",
    "\n",
    "            input_seq_len = self.config.input_seq_len\n",
    "            step_size = self.config.step_size\n",
    "\n",
    "            for code in tqdm.tqdm(region_codes, desc=\"Transforming TS Data\"):\n",
    "                ts_one = ts_data[ts_data['sub_region_code'] == code].sort_values(by='date')\n",
    "                indices = self._get_cutoff_indices(ts_one, input_seq_len, step_size)\n",
    "\n",
    "                x = np.zeros((len(indices), input_seq_len), dtype=np.float64)\n",
    "                y = np.zeros((len(indices)), dtype=np.float64)\n",
    "                date_hours, temps = [], []\n",
    "\n",
    "                for i, (start, mid, end) in enumerate(indices):\n",
    "                    x[i, :] = ts_one.iloc[start:mid]['demand'].values\n",
    "                    y[i] = ts_one.iloc[mid]['demand']\n",
    "                    date_hours.append(ts_one.iloc[mid]['date'])\n",
    "                    temps.append(ts_one.iloc[mid]['temperature_2m'])\n",
    "\n",
    "                features_one = pd.DataFrame(\n",
    "                    x,\n",
    "                    columns=[f'demand_prev_{i+1}_hr' for i in reversed(range(input_seq_len))]\n",
    "                )\n",
    "                features_one['date'] = date_hours\n",
    "                features_one['sub_region_code'] = code\n",
    "                features_one['temperature_2m'] = temps\n",
    "\n",
    "                targets_one = pd.DataFrame(y, columns=['target_demand_next_hour'])\n",
    "\n",
    "                features = pd.concat([features, features_one], ignore_index=True)\n",
    "                targets = pd.concat([targets, targets_one], ignore_index=True)\n",
    "\n",
    "            return features, targets['target_demand_next_hour']\n",
    "\n",
    "    def preprocess_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        try:\n",
    "            validation_status = self.check_status()\n",
    "        \n",
    "            if not validation_status:\n",
    "                logger.error(\"Data validation failed. Skipping data cleaning.\")\n",
    "            logger.info(f\"Validation Status : {validation_status}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            train_x, train_y = self.transform_ts_data_into_features_and_target(train_df)\n",
    "            test_x, test_y = self.transform_ts_data_into_features_and_target(test_df)\n",
    "\n",
    "            # Save numpy arrays\n",
    "            np.save(self.config.x_transform, train_x.values)\n",
    "            np.save(self.config.y_transform, train_y.values)\n",
    "\n",
    "            # Save as CSV\n",
    "            train_x.to_csv(self.config.train_features, index=False)\n",
    "            train_y.to_csv(self.config.train_target, index=False)\n",
    "            test_x.to_csv(self.config.test_features, index=False)\n",
    "            test_y.to_csv(self.config.test_target, index=False)\n",
    "\n",
    "            logger.info(\"Feature transformation and saving completed.\")\n",
    "            return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c4364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 08:52:12,382: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-05 08:52:12,398: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-05 08:52:12,412: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-05 08:52:12,688: INFO: helpers: created directory at: artifacts\\data_transformation]\n",
      "[2025-07-05 08:52:12,707: INFO: helpers: binary file saved at: artifacts\\data_transformation\\label_encoder.pkl]\n",
      "[2025-07-05 08:52:12,708: INFO: 364925033: Basic preprocessing completed.]\n",
      "[2025-07-05 08:52:12,771: INFO: 364925033: Feature engineering completed.]\n",
      "[2025-07-05 08:52:12,808: INFO: 364925033: Train size: (80245, 9), Test size: (20086, 9)]\n",
      "[2025-07-05 08:52:12,824: ERROR: 364925033: Error reading validation status: Expecting value: line 1 column 1 (char 0)]\n",
      "[2025-07-05 08:52:12,824: ERROR: 364925033: Data validation failed. Skipping data cleaning.]\n",
      "[2025-07-05 08:52:12,824: INFO: 364925033: Validation Status : False]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming TS Data: 100%|██████████| 11/11 [00:20<00:00,  1.89s/it]\n",
      "Transforming TS Data: 100%|██████████| 11/11 [00:03<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 08:53:33,142: INFO: 364925033: Feature transformation and saving completed.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    train_df, test_df = data_transformation.train_test_splitting()\n",
    "    (train_x, train_y), (test_x, test_y) = data_transformation.preprocess_features(train_df, test_df)\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
